{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UinnQjUTcBLX",
    "outputId": "393ed085-60d0-446a-dbbc-1fbc1c415160"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "تجربہکار ہندوستانی آفس پنر روی چندر نا یش ون نے آئند ہا یش یا ء کپ 2 0 2 3 ء کی غیریقینی قسمت پراپن ی رائے کااظہار کیاہے جوپ اکستا نمی ں ہونے جارہاہے اپنے ی وٹ ی و بچی نل پربا تکر تے ہوئے روی چندر نا یش ون نےکہا کہا گرپ ڑ وسیم لک بھارت ایشیا کپ 2 0 2 3 ء میں شرکتکرنا چاہتاہے توم قامت بدیل کردینا چاہیے\n",
      "آفس پنر نےکہا کہان ٹرن یش نل کرکٹ کونسل ( آئیس ی سی ) نے پاکستان کوٹ ورن امن ٹکی میزبانی کا حقد ے دیاہے لیکن بھارت پاکستان کا دورہ کرنے کوت یار نہیں روی چندر نا یش ون نے بھی 2 0 2 3 ء میں 5 0 اوور کے ورلڈکپ کے حوالے سے پاکستان کرکٹبورڈ ( پیس ی بی ) کے حالیہ بیان کاجو ابدیت ے ہوئے کہا میرے خیال میںیہ ممکن نہیںہے آفس پنر نے مزید کہاکہ پاکستان نے پہلے بھارت کا دورہ کرنے سے انکار کردیا تھالیکن آ خرکار وہمی گا ایونٹس میں شرکت کے لیے بھارت گئے تھے\n",
      "غورطلب ہے کہا یش یاک پکے معاملے پر حال ہی میں بحرین میں ہونے والی ایک ہنگامی میٹنگ میں غور کیاگیا جہاں وین ی و کے بارےمیں حتمی فیصلہ مارچ تک موخر کردیاگیا بحرین میں ایشین کرکٹ کونسل ( اے سیس ی ) کے اجلاس کے بعد بیس ی سی آئی کے حکام نے اعلان کیاکہ بورڈ نے ایونٹ کے لیے اپنی ٹیم پاکستان نہب ھی جنے کافیصلہ کیاہے تاہم پیس ی بیکے حکام نے بھیس ختر د عمل کااظہار کرتے ہوئے کہاہے کہو ہاک توب رمی ں بھارت میں ہونے والے ورلڈکپ 2 0 2 3 ء میں شرکت نہیں کریںگے\n",
      "توشہخانہ کیس میں عمرانخان پر فردجرم عائد نہہ وسکی سیشن عدالت نے الیکشن کمیشن کیچ ی ئ رمی نپ ی ٹی آئی کیخلاف فوجداری مقدمے کید رخ وا ست پر عمرانخان کی طبیب نیا دوں پر آ ج حاضری سےاس ت ث ن ٰ ی کید رخ وا ستم نظور کرلیا لیک ش نکمی ش نکی جانب سے پیٹیآئی وکلا کوت صدیق شدہ کاپیاں فراہم نہیں کی گئیں تصدیق شدہ کاپیاں کیفر اہم ی کے بعد فردجرم کیا گلی تاریخ مقرر کی جائےگی\n",
      "تفصیلات کے مطابق سیشن عدالت میں الیکشن کمیشن کی عمرانخان کے خلاف فوجداری مقدمے کید رخ وا ست پرس ماع تہ و ئی جج ظفر اقبال نے درخواست پرس ماع تکی عمرانخان کی جانب سے طبیب نیا دوں پر حاضری سےاس ت ثنی ٰ کید رخ وا ستد ائر کی گئی عدالت نے وکیل سےاس تف سارک یاک ہک یام چلک ے جمع کروا دیئے وکیل گوہر علیخان نےکہا کہ عمرانخان کے ضمانتی مچلکے جمع کروا دیئے جج نےاس تف سارک یاک ہا ی سے حاضری سےاس ت ث ن ٰ ی کید رخ وا ستد ائر ہوتی رہی توفر دج رم کیسے عائدہ وگ ی وکیل علی بخاری نے بتایاکہ ہمیں مصدقہ کاپیاں فراہم نہیں کی گئیں\n",
      "جج نے ہدایت کیک ہت مام ثبوتوں کی تصدیق شدہ کاپیاں عدالت اور پیٹیآئی کو فراہم کریں وکیل الیکشن کمیشن نےکہا کہہ م آ جہ ی کمپ لین ٹاور ثبوتوں کیم صدقہ کاپیاں فراہم کردیں گے وکیل الیکشن کمیشن نےکہا کہ عمرانخان ابھیتک عدالت کیوںنہیں آئے جج نےاس تف سارک یاک ہک یاک ہم جھ ے ایکتاری خبت ادی ں عمرانخان کب عدالت آئیں گے عمرانخان کے وکیل نےکہا کہ عمرانخان کی صحت نے اجازت دیت و آئیں گے ڈاکٹرز کیہ دا یت پرعمل کررہے ہیں عدالت نے عمرانخان کی آ ج حاضری سےاس ت ثنی ٰ پر فیصلہ محفوظ کرلیا عدالت نے طبیب نیا دوں پر عمرانخان کی آ ج حاضری سےاس ت ثنی ٰ کید رخ وا ستم نظور کرلی چیئرمین پیٹیآئی عمرانخان پر فردجرم عائد نہہ وسکی\n",
      "یاد رہے کہ توشہخانہ کیس میں عمرانخان نے سینئر وکیل خواجہ حارث کی خدمات حاصل کرلیں خواجہ حارث سمیت 4 وکلا کے وکالت نامے عدالت میں جمع کرادی ئے گئے ہیں\n",
      "سابق وفاقیوزیر مفتاح اسماعیل کاکہنا ہے کہو زیر خزانہ اسحاقڈار نے پھروہی حرکتکی جوش و کت ترین نے کی تھیا نہو ں نے آئی ایما یف سے کیاگیا معاہدہ توڑا تھا انہوںنے ہمن ی و زک ے پروگرام میں گفتگو کرتے ہوئے مزید کہاکہ آئی ایما یف سے معاہدہ ہونے کے بعد چیزیں بہتر ہوں گیپ ی آئی اے اس سال 9 0 ارب روپے کان قصا نکر ے گیلی کنا گرپ ی آئی اےک ین جک اری ہوگی تو 9 0 ارب روپے کان قصا نن ہیں ہوگا\n",
      "مفتاح اسماعیل نےکہا کہ میں تو چاہ رہاتھا پہلے دن ہی پٹرولیم مصنوعات کی قیمتوں میں اضافہ کیاجائے سیلاب آنے سے پہلے ہمنے ڈیفالٹ رسک کوک مکی ات ھا شہبازشریف اگر آ جوزی را عظم نہہوت ے توم لک ڈیفالٹ کیطرف جاچکا تھا قبلازیں ایک بیان میں مفتاح اسماعیل نےکہا کہہ مسلملیگ میں کوئی فارورڈ بلاک نہیں بن رہا پاکستان ڈیفالٹ نہیںکرےگا پارٹی کے موجودہ نائبصدر کو پارٹی کے فیصلے کرنے کا اختیار حاصل ہے وہ گزشتہ روز کراچی میں نیشنل اسلامک اکنامک کانفرنس سے خطاب کررہے تھے\n",
      "سابق وزیر خزانہ نے اپنے خطاب اور میڈیا سے باتچیت کے دوران مسلملیگ میں فارورڈ بلاک بننے کی خبروں کوم ستر دک رت ے ہوئے کہاکہ ایسی اطلاعات میں کوئی صداقت نہیںہے شاہد خاقان عباسی پہلےہی کہہ چکے ہیںکہ وہپ ار ٹیمیں ہی ہیںانہوں نے اپنے پارٹی عہدہ سےاس تعف ی ٰ دیاہے پارٹی نہیں چھوڑی ہے ملک کیم عا شی صورتحال کے حوالے سے مفتاح اسماعیل کاکہنا تھاکہ ملک کے ڈیفالٹ ہونے کاکوئی امکان ہیں ا ٓ ئی ایما یف سے ڈیل ہوجا ئی گیا ورم عامل ات درست ہوجا ئی نگے انکا کہناتھا کہا گر شوکت ترین فروری میں ا ٓ ئی ایما یف کام عاہدہ نہ توڑتے تو حالات اچھے ہوتے پیٹیا ٓ ئی کی حکومت میں 8 0 ارب ڈالر کیا م پورٹ ہوئی ڈالر کورو ککر رکھنا غلطی تھیا ورا سل ئے ڈالر کی قیمتا ٓ جبل ند ترین سطح پرہ ے ڈالر کی قدر کو پکڑ کر روکا نہیں جاسکتا ڈالر کافری فلوٹ رہنا ہی بہتر ہے غیرضروری مشینری کوہ منے امپورٹ سے روکا تھا ابھی جوپ ابن دی ہے وہکچھ اور ہے\n",
      "وزیرداخلہ رانا ثناءاللہ کاکہنا ہے کہ عمرانخان نے جیل بھرو تحریک کااعلان کیاہے وہپ ہلے بھیا سہ تھکن ڈے میں ناکام ہوئے عمرانخان کوم علوم ہینہ ی ں کہ جیل میں رہنا کتنا مشکل ہے میڈیا رپورٹس کے مطابق رانا ثناءاللہ نے اپنے بیان میں کہاکہ عمرانخان کام قصد سیاسی افراتفری ہے وہا سمیں ناکام ہوںگے عمرانخان اپنی زندگی کا صرف ایکدن جیل میں رہے\n",
      "اے پیس یم ی ں تمام جماعتوں کیش رکت کے لیے 2 دنب ڑ ھادی ئے ہیں اے پیس یم ی ں کسی جماعت کے شامل نہہ ون ے سے معاملہ رکن ہیں سکتا وزیرداخلہ کاکہنا تھاکہ قومی معاملے کاتم امس یاس ی جماعتوں کومل کر حل نکالنا ہوگا اے پیس یم ی ں اتفاق رائے کے بعد حکومت دہشتگردوں کے خلاف کارروائی کافیصلہ کرے گی\n",
      "قبلازیں وفاقیوزیر ایاز صادق نےکہا کہ پرویز خٹک اسد قیصر اورا ع جاز شاہ کوا ے پیس یم ی ں شرکت کید ع وتد یان سے کہاکہ وہی ہد ع و تعمر ان خان تک پہنچا دیں\n",
      "ایاز صادق نے پیٹیآئی رہنما اسد قیصر کے بیان پر رد ِ عمل دیتے ہوئے کہاہے کہا ے پیس یم ی ں شرکت کیلئے کسیکو بھید ع و تنا مہ نہیں بھجوایا فونک ی ے یام لا قا تکر کے کانفرنس میں شرکت کید ع وتد ی گئی ہے سابقا سپیکر قومی اسمبلی اسد قیصر رویوں کی بات نہک ری ں توا چھا ہے پیٹیآئی نے نلی گپی پل زپ ار ٹیکے ساتھ ایف آئی اے اور نیب کے ذریعے کیاکچھ نہیں کیا جبکہ تحریک انصاف کے رہنما اسد قیصر کاکہنا تھاکہ مجھے حکومتی نمائندوں نے فون پرا ے پیس یم ی ں شرکت کید ع وتد ی کانفرنس کید ع وتد ین ے کایہ بالکل بھیم ناس بطریق ہن ہیں ہے اے پیس یم ی ں دعوت دینا کا احسن طریقہ ہوتا ہے ہمس مجھ تے ہیںکہ ملک سنگین بحرانوں کاشکار ہے اوراس وقتی کج ہت ی کی ضرورت ہے حکومتی رویہ غیرآئینی ہے اوراس ماحول میں ی کج ہت ی کا سوال ہیپی دانہ ی ں ہوتا حکومت کوس بسے پہلے اپنے رویے میں بہتری لانا ہوگی ہمنے اپنے دورمیں دہشتگردی پر قابو پانے کیلئے بہترین حکمتعملی بناکر امن قائم کیا\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import ngrams\n",
    "with open(\"words_test.txt\", \"r\", encoding=\"utf-8\") as input_file:\n",
    "    content = [line.strip() for line in input_file]\n",
    "with open(\"words.txt\", \"r\", encoding=\"utf-8\") as dictionaryy:\n",
    "    urdu = dictionaryy.read()\n",
    "with open(\"bigram_words.txt\", \"r\", encoding=\"utf-8\") as dictionary:\n",
    "    urdu1 = dictionary.read()\n",
    "\n",
    "Complete_Dict = open(\"Complete_Dictionary.txt\",'w',encoding=\"utf-8\")\n",
    "Complete_Dict.write(urdu)\n",
    "Complete_Dict.close()\n",
    "Complete_Dict = open(\"Complete_Dictionary.txt\",'a',encoding=\"utf-8\")\n",
    "Complete_Dict.write(urdu1)\n",
    "\n",
    "F = open(\"Complete_Dictionary.txt\",'r',encoding=\"utf-8\")\n",
    "N = open(\"Updated_Dictionary.txt\",'w',encoding=\"utf-8\")\n",
    "for line in F:\n",
    "    modified = line.replace(\"_\",\"\")\n",
    "    N.write(modified)\n",
    "\n",
    "readd = open(\"Updated_Dictionary.txt\",'r',encoding=\"utf-8\")\n",
    "WD = readd.read()\n",
    "Whole_Dictionary = WD.strip().split()\n",
    "\n",
    "def replace_ngrams(sentence, n):\n",
    "    words = []\n",
    "    fixed_text = []\n",
    "    words = list(sentence)\n",
    "    flag = False\n",
    "     # taking note of the length\n",
    "    # to check that each word/sentence is catered.\n",
    "    count = 0\n",
    "    # breaking the sentence in to words\n",
    "    len_of_word = len(words)\n",
    "    count = 0\n",
    "    while count < len_of_word:\n",
    "        flag = False\n",
    "        for i in n:\n",
    "          # i is basically the nth gram we are sending from the driver code\n",
    "            nth_gram = ''.join(words[count:count + i])\n",
    "            # slicing on the behalf of specified n gram\n",
    "            if nth_gram in Whole_Dictionary:\n",
    "              # now matching that if we found that in our dictionary then we'll append it in our fiex_text wali list\n",
    "                fixed_text.append(nth_gram)\n",
    "                count = count + i\n",
    "                # if found then it will be incremented on by itself\n",
    "                flag = True\n",
    "                # once found break the loop\n",
    "                break\n",
    "        if not flag:\n",
    "          # if not found the word will remain the same and be placed inside the result list\n",
    "            fixed_text.append(words[count])\n",
    "            # in this case the index will be one only\n",
    "            count = count + 1\n",
    "\n",
    "    return ' '.join(fixed_text)\n",
    "    # as result is a list it .join will convert that to sentences\n",
    "\n",
    "# all numbers of n grams i want to make of the text\n",
    "ns = [9, 8, 7, 6, 5, 4, 3, 2]\n",
    "for line in content:\n",
    "    print(replace_ngrams(line, ns))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8U7EFtMascp",
    "outputId": "8ecf7e8e-cd62-4c21-9fd0-8794a2664fb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mein oniversiti jati hon ise lehe thak jate hon zrori nahii ke wsi thak jao . Iske aellawa prehhna aecha neh lagta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Task 3\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "# this function alone is working perfectly\n",
    "def replace_ain_with_ein(word):\n",
    "    # Check if the word ends with \"ain\"\n",
    "    if word.endswith(\"ain\"):\n",
    "        # Replace \"ain\" with \"ein\"\n",
    "        return word[:-3] + \"ein\"\n",
    "    else:\n",
    "        # If not, return the word as is\n",
    "        return word\n",
    "\n",
    "# this function is also working perfectly\n",
    "def replace_ar_with_r(word):\n",
    "\n",
    "    if \"ar\" in word[1:]:\n",
    "        return word.replace(\"ar\", \"r\")\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "\n",
    "# this function also works accurately if applied alag se\n",
    "def replace_ai_with_ae(word):\n",
    "    return word.replace(\"ai\", \"ae\")\n",
    "\n",
    "\n",
    "# perfectly working\n",
    "def replace_iy_with_I(word):\n",
    "  # re.sub is an function using regular expression r'i(y+)' to match 'I' by multiple y's\n",
    "    return re.sub(r'i(y+)', 'I', word)\n",
    "\n",
    "# working\n",
    "def replace_ay_e(word):\n",
    "  if word.endswith(\"ay\"):\n",
    "     return word[:-2] + \"e\"\n",
    "\n",
    "  else:\n",
    "        return word\n",
    "        # else returning the same word\n",
    "\n",
    "def replace_ih_with_eh(word):\n",
    "    return re.sub(r'i(h+)', 'eh', word)\n",
    "\n",
    "def replace_es_with_is(word):\n",
    "    if word.startswith(\"es\"):\n",
    "        return \"is\" + word[2:]\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "\n",
    "def replace_ey_with_e(word):\n",
    "  if word.endswith(\"ey\"):\n",
    "    return word[:-2] + \"e\"\n",
    "  else:\n",
    "    return word\n",
    "\n",
    "def replace_multiple_s(word):\n",
    "    new_word = [word[0]]  # Initialize the new word with the first character\n",
    "    for i in range(1, len(word)):\n",
    "        # Check if the current character is not 's' or if it's 's' but different from the previous character\n",
    "        if word[i] != 's' or word[i] == 's' and word[i] != word[i - 1]:\n",
    "            new_word.append(word[i])\n",
    "    return ''.join(new_word)\n",
    "\n",
    "def replace_multiple_a(word):\n",
    "    new_word = [word[0]]  # Initialize the new word with the first character\n",
    "    for i in range(1, len(word)):\n",
    "        # Check if the current character is not 's' or if it's 's' but different from the previous character\n",
    "        if word[i] != 'a' or word[i] == 'a' and word[i] != word[i - 1]:\n",
    "            new_word.append(word[i])\n",
    "    return ''.join(new_word)\n",
    "\n",
    "def replace_multiple_o(word):\n",
    "    new_word = [word[0]]  # Initialize the new word with the first character\n",
    "    for i in range(1, len(word)):\n",
    "        # Check if the current character is not 'o' or if it's 'o' but different from the previous character\n",
    "        if word[i] != 'o' or word[i] == 'o' and word[i] != word[i - 1]:\n",
    "            new_word.append(word[i])\n",
    "    return ''.join(new_word)\n",
    "\n",
    "def replace_multiple_d(word):\n",
    "    new_word = [word[0]]  # Initialize the new word with the first character\n",
    "    for i in range(1, len(word)):\n",
    "        # Check if the current character is not 'd' or if it's 'd' but different from the previous character\n",
    "        if word[i] != 'd' or word[i] == 'd' and word[i] != word[i - 1]:\n",
    "            new_word.append(word[i])\n",
    "    return ''.join(new_word)\n",
    "\n",
    "\n",
    "def replace_multiple_j(word):\n",
    "    new_word = [word[0]]  # Initialize the new word with the first character\n",
    "    for i in range(1, len(word)):\n",
    "        # Check if the current character is not 's' or if it's 's' but different from the previous character\n",
    "        if word[i] != 'j' or word[i] == 'j' and word[i] != word[i - 1]:\n",
    "            new_word.append(word[i])\n",
    "    return ''.join(new_word)\n",
    "\n",
    "\n",
    "def replace_ie_with_y(word):\n",
    "  if word.endswith(\"ie\"):\n",
    "    return word[:-2] + \"y\"\n",
    "  else:\n",
    "    return word\n",
    "\n",
    "def replace_ry_with_ri(word):\n",
    "    if len(word) <= 2:\n",
    "        return word\n",
    "\n",
    "    result = word[0]\n",
    "    for i in range(1, len(word)):\n",
    "\n",
    "          if word[i] == 'r' and word[i + 1] == 'y':\n",
    "              result += word[i]\n",
    "              result += 'i'\n",
    "          else:\n",
    "            if word[i] == 'y':\n",
    "              continue;\n",
    "            result += word[i]\n",
    "\n",
    "    return result\n",
    "\n",
    "def replace_sy_ty_with_si_ti(word):\n",
    "\n",
    "    if len(word) <= 2:\n",
    "        return word\n",
    "\n",
    "    result = word[0]\n",
    "    for i in range(1, len(word)):\n",
    "        if (word[i] == 's' and word[i + 1] == 'y') or (word[i] == 't' and word[i + 1] == 'y'):\n",
    "            k = word[i] + 'i'\n",
    "            result += k\n",
    "\n",
    "        else:\n",
    "          if word[i] == 'y':\n",
    "            continue;\n",
    "          result += word[i]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def replace_multiple_ee_with_i(word):\n",
    "    return word.replace(\"ee\", \"i\")\n",
    "\n",
    "\n",
    "\n",
    "def lemmatizer(word):\n",
    "\n",
    "    if word:\n",
    "      word = replace_ain_with_ein(word)\n",
    "      word = replace_ay_e(word)\n",
    "      word = replace_ar_with_r(word)\n",
    "      word = replace_ai_with_ae(word)\n",
    "      word = replace_sy_ty_with_si_ti(word)\n",
    "      word = replace_iy_with_I(word)\n",
    "      word = replace_ih_with_eh(word)\n",
    "      word = replace_ey_with_e(word)\n",
    "      word = replace_multiple_s(word)\n",
    "      word = replace_ie_with_y(word)\n",
    "      word = replace_ry_with_ri(word)\n",
    "      word = replace_es_with_is(word)\n",
    "      word = replace_multiple_a(word)\n",
    "      word = replace_multiple_o(word)\n",
    "      word = replace_multiple_d(word)\n",
    "      word = replace_multiple_j(word)\n",
    "      word = replace_multiple_ee_with_i(word)\n",
    "      word = word.replace(\"u\",\"o\")\n",
    "\n",
    "    return word\n",
    "\n",
    "sentence = \"Main universyty jaty hun isey lehyay thaak jaaatey hun zaroori nahii kay wysy thaak jaaaoo. Iske aillaawa parehhna aichaa nihh lagtaa\"\n",
    "new_words = word_tokenize(sentence)\n",
    "lemmatized = [lemmatizer(word) for word in new_words]\n",
    "\n",
    "lemmatized = ' '.join(lemmatized)\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SNsp2h6VhklI",
    "outputId": "fcfec649-4ce5-4c10-8792-44ba4b40a329"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('arabic', 0),\n",
       " ('azerbaijani', 3),\n",
       " ('basque', 0),\n",
       " ('bengali', 0),\n",
       " ('catalan', 3),\n",
       " ('chinese', 0),\n",
       " ('danish', 0),\n",
       " ('dutch', 5),\n",
       " ('english', 8),\n",
       " ('finnish', 0),\n",
       " ('french', 1),\n",
       " ('german', 0),\n",
       " ('greek', 0),\n",
       " ('hebrew', 0),\n",
       " ('hinglish', 11),\n",
       " ('hungarian', 1),\n",
       " ('indonesian', 1),\n",
       " ('italian', 2),\n",
       " ('kazakh', 0),\n",
       " ('nepali', 0),\n",
       " ('norwegian', 0),\n",
       " ('portuguese', 1),\n",
       " ('romanian', 1),\n",
       " ('russian', 0),\n",
       " ('slovene', 0),\n",
       " ('spanish', 1),\n",
       " ('swedish', 0),\n",
       " ('tajik', 0),\n",
       " ('turkish', 0)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task 2\n",
    "# the code is not dealing with the repeated stopwords\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "Identified_Languages = []\n",
    "text =\"An article is qualunque member van un class of dedicated words naquele estão used with noun phrases per mark the identifiability of the referents of the noun phrases\"\n",
    "words = word_tokenize(text)\n",
    "count = 0\n",
    "for language in stopwords.fileids():\n",
    "  for word in words:\n",
    "    if word in stopwords.words(language):\n",
    "      count = count + 1\n",
    "  Identified_Languages.append((language,count))\n",
    "  count = 0\n",
    "\n",
    "Identified_Languages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1WfZXg0UEwpH",
    "outputId": "b6a4ff44-abeb-4fde-dc0f-6597759a7079"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of\n",
      "of\n",
      "of\n",
      "with\n",
      "is\n",
      "the\n",
      "the\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "english = set(stopwords.words('english'))\n",
    "\n",
    "for sw in english:\n",
    " for word in words:\n",
    "  if word==sw:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qXr45-zyKxRZ",
    "outputId": "f4c049be-83d6-469b-8260-a68d37ef3bbe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Zainr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " غمخوار کے پاس جگرِ تشنۂ آزار ،\n",
      " تسلی نہ ہوا میں نے وہ شیر پھر ہوشیار\n",
      " ہوگا کیا مرا تذکرہ جو ساقی نے بادہ خواروں کی\n",
      " انجمن میں تو پیر مے خانہ ہر کوئی\n",
      " \n",
      "\n",
      " بادہ خوار ہوگا کبھی جو آوارۂ جنوں تھے وہ\n",
      " بستیوں میں آ بسیں گے برہنہ پائی وہی\n",
      " رہے گی مگر نیا خار زار ہوگا سنا دیا گوش\n",
      " منتظر کو حجاز کی خامشی نے آخر جو عہد صحرائیوں\n",
      " \n",
      "\n",
      " سے باندھا گیا تھا پر استوار ہوگا نکل\n",
      " کے صحرا سے جس نے روما کی\n",
      " سلطنت کو الٹ دیا تھا سنا ہے\n",
      " یہ قدسیوں سے میں نے وہ شیر پھر\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk import word_tokenize\n",
    "from nltk import ngrams\n",
    "nltk.download('punkt')\n",
    "\n",
    "file_content = open('ghalib.txt', encoding=\"utf8\").read()\n",
    "file_content += open('iqbal.txt', encoding=\"utf8\").read()\n",
    "    \n",
    "words = word_tokenize(file_content)\n",
    "\n",
    "tgms = nltk.ngrams(words, 3)\n",
    "\n",
    "ugms = nltk.ngrams(words, 1)\n",
    "\n",
    "ugmsFD = nltk.FreqDist(ugms)\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist()     # making empty dictionary of conditional freq dist\n",
    "\n",
    "# cfd[(w1, w2)][w3] is returning the frequency of the trigram (w1, w2, w3)\n",
    "for w1, w2, w3 in tgms:\n",
    "    cfd[(w1, w2)][w3] += 1\n",
    "    \n",
    "for w1_w2 in cfd:\n",
    "    count = float(sum(cfd[w1_w2].values()))    # here in this line we are finding the frequency of the first two words of the trigramt\n",
    "    for w3 in cfd[w1_w2]:       # for each 3rd word of the trigram         \n",
    "        cfd[w1_w2][w3] /= count       # dividing the frequency of the trigram by the frequency of the first two words of the trigram to find the probability of the trigram\n",
    "\n",
    "words_to_start = random.choice(list(cfd.keys()))    # select random words to use as starting text of first verse\n",
    "\n",
    "rand = random.randint(7, 10)    # to generate 7-10 words per verse\n",
    "text_generated = \"\"\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        for k in range(rand):\n",
    "            next_word = cfd[words_to_start].max()         # generate the next word based on the max freq\n",
    "            text_generated = text_generated + \" \" + next_word\n",
    "            words_to_start = words_to_start[1], next_word           # update the first word of the verse with the second word of the previous verse\n",
    "\n",
    "        text_generated = text_generated + \"\\n\"\n",
    "        rand = random.randint(7, 10)        # generate random number of words for next verse\n",
    "        \n",
    "    print (text_generated, \"\\n\")          # print a blank line after each stanza\n",
    "    text_generated = \"\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
